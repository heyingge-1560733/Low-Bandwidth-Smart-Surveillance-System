{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vL2VrLjtHZCs"
      },
      "source": [
        "# Overview\n",
        "\n",
        "This Colab helps validate a training set for the NN classifier soultion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8OxqytxxV-e"
      },
      "source": [
        "# Step 0: Start Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGJFajURxZii"
      },
      "source": [
        "Connect the Colab to hosted Python3 runtime (check top-right corner) and then install required dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsA8WJi60PaX",
        "outputId": "05a12563-d009-4925-9c04-bee163273534"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mediapipe==0.8.10\n",
            "  Downloading mediapipe-0.8.10-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 32.8 MB 275 kB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mediapipe==0.8.10) (3.2.2)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (from mediapipe==0.8.10) (4.1.2.30)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mediapipe==0.8.10) (1.21.6)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from mediapipe==0.8.10) (21.4.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from mediapipe==0.8.10) (1.0.0)\n",
            "Requirement already satisfied: protobuf>=3.11.4 in /usr/local/lib/python3.7/dist-packages (from mediapipe==0.8.10) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.11.4->mediapipe==0.8.10) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe==0.8.10) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe==0.8.10) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe==0.8.10) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe==0.8.10) (1.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mediapipe==0.8.10) (4.2.0)\n",
            "Installing collected packages: mediapipe\n",
            "Successfully installed mediapipe-0.8.10\n"
          ]
        }
      ],
      "source": [
        "!pip install pillow\n",
        "!pip install matplotlib\n",
        "!pip install numpy\n",
        "!pip install opencv-python\n",
        "!pip install tqdm\n",
        "!pip install mediapipe==0.8.10\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7S1Dl8Dhfa2"
      },
      "source": [
        "# Codebase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L6HLFd9AXmh"
      },
      "source": [
        "## Pose embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QBrKOeP30RAx"
      },
      "outputs": [],
      "source": [
        "class FullBodyPoseEmbedder(object):\n",
        "  \"\"\"Converts 3D pose landmarks into 3D embedding.\"\"\"\n",
        "\n",
        "  def __init__(self, torso_size_multiplier=2.5):\n",
        "    # Multiplier to apply to the torso to get minimal body size.\n",
        "    self._torso_size_multiplier = torso_size_multiplier\n",
        "\n",
        "    # Names of the landmarks as they appear in the prediction.\n",
        "    self._landmark_names = [\n",
        "        'nose',\n",
        "        'left_eye_inner', 'left_eye', 'left_eye_outer',\n",
        "        'right_eye_inner', 'right_eye', 'right_eye_outer',\n",
        "        'left_ear', 'right_ear',\n",
        "        'mouth_left', 'mouth_right',\n",
        "        'left_shoulder', 'right_shoulder',\n",
        "        'left_elbow', 'right_elbow',\n",
        "        'left_wrist', 'right_wrist',\n",
        "        'left_pinky', 'right_pinky',\n",
        "        'left_index', 'right_index',\n",
        "        'left_thumb', 'right_thumb',\n",
        "        'left_hip', 'right_hip',\n",
        "        'left_knee', 'right_knee',\n",
        "        'left_ankle', 'right_ankle',\n",
        "        'left_heel', 'right_heel',\n",
        "        'left_foot_index', 'right_foot_index',\n",
        "    ]\n",
        "\n",
        "  def __call__(self, landmarks):\n",
        "    \"\"\"Normalizes pose landmarks and converts to embedding\n",
        "    \n",
        "    Args:\n",
        "      landmarks - NumPy array with 3D landmarks of shape (N, 3).\n",
        "\n",
        "    Result:\n",
        "      Numpy array with pose embedding of shape (M, 3) where `M` is the number of\n",
        "      pairwise distances defined in `_get_pose_distance_embedding`.\n",
        "    \"\"\"\n",
        "    assert landmarks.shape[0] == len(self._landmark_names), 'Unexpected number of landmarks: {}'.format(landmarks.shape[0])\n",
        "\n",
        "    # Get pose landmarks.\n",
        "    landmarks = np.copy(landmarks)\n",
        "\n",
        "    # Normalize landmarks.\n",
        "    landmarks = self._normalize_pose_landmarks(landmarks)\n",
        "\n",
        "    # Get embedding.\n",
        "    embedding = self._get_pose_distance_embedding(landmarks)\n",
        "\n",
        "    return embedding\n",
        "\n",
        "  def _normalize_pose_landmarks(self, landmarks):\n",
        "    \"\"\"Normalizes landmarks translation and scale.\"\"\"\n",
        "    landmarks = np.copy(landmarks)\n",
        "\n",
        "    # Normalize translation.\n",
        "    pose_center = self._get_pose_center(landmarks)\n",
        "    landmarks -= pose_center\n",
        "\n",
        "    # Normalize scale.\n",
        "    pose_size = self._get_pose_size(landmarks, self._torso_size_multiplier)\n",
        "    landmarks /= pose_size\n",
        "    # Multiplication by 100 is not required, but makes it easier to debug.\n",
        "    landmarks *= 100\n",
        "\n",
        "    return landmarks\n",
        "\n",
        "  def _get_pose_center(self, landmarks):\n",
        "    \"\"\"Calculates pose center as point between hips.\"\"\"\n",
        "    left_hip = landmarks[self._landmark_names.index('left_hip')]\n",
        "    right_hip = landmarks[self._landmark_names.index('right_hip')]\n",
        "    center = (left_hip + right_hip) * 0.5\n",
        "    return center\n",
        "\n",
        "  def _get_pose_size(self, landmarks, torso_size_multiplier):\n",
        "    \"\"\"Calculates pose size.\n",
        "    \n",
        "    It is the maximum of two values:\n",
        "      * Torso size multiplied by `torso_size_multiplier`\n",
        "      * Maximum distance from pose center to any pose landmark\n",
        "    \"\"\"\n",
        "    # This approach uses only 2D landmarks to compute pose size.\n",
        "    landmarks = landmarks[:, :2]\n",
        "\n",
        "    # Hips center.\n",
        "    left_hip = landmarks[self._landmark_names.index('left_hip')]\n",
        "    right_hip = landmarks[self._landmark_names.index('right_hip')]\n",
        "    hips = (left_hip + right_hip) * 0.5\n",
        "\n",
        "    # Shoulders center.\n",
        "    left_shoulder = landmarks[self._landmark_names.index('left_shoulder')]\n",
        "    right_shoulder = landmarks[self._landmark_names.index('right_shoulder')]\n",
        "    shoulders = (left_shoulder + right_shoulder) * 0.5\n",
        "\n",
        "    # Torso size as the minimum body size.\n",
        "    torso_size = np.linalg.norm(shoulders - hips)\n",
        "\n",
        "    # Max dist to pose center.\n",
        "    pose_center = self._get_pose_center(landmarks)\n",
        "    max_dist = np.max(np.linalg.norm(landmarks - pose_center, axis=1))\n",
        "\n",
        "    return max(torso_size * torso_size_multiplier, max_dist)\n",
        "\n",
        "  def _get_pose_distance_embedding(self, landmarks):\n",
        "    \"\"\"Converts pose landmarks into 3D embedding.\n",
        "\n",
        "    We use several pairwise 3D distances to form pose embedding. All distances\n",
        "    include X and Y components with sign. We differnt types of pairs to cover\n",
        "    different pose classes. Feel free to remove some or add new.\n",
        "    \n",
        "    Args:\n",
        "      landmarks - NumPy array with 3D landmarks of shape (N, 3).\n",
        "\n",
        "    Result:\n",
        "      Numpy array with pose embedding of shape (M, 3) where `M` is the number of\n",
        "      pairwise distances.\n",
        "    \"\"\"\n",
        "    embedding = np.array([\n",
        "        # One joint.\n",
        "\n",
        "        self._get_distance(\n",
        "            self._get_average_by_names(landmarks, 'left_hip', 'right_hip'),\n",
        "            self._get_average_by_names(landmarks, 'left_shoulder', 'right_shoulder')),\n",
        "\n",
        "        self._get_distance_by_names(landmarks, 'left_shoulder', 'left_elbow'),\n",
        "        self._get_distance_by_names(landmarks, 'right_shoulder', 'right_elbow'),\n",
        "\n",
        "        self._get_distance_by_names(landmarks, 'left_elbow', 'left_wrist'),\n",
        "        self._get_distance_by_names(landmarks, 'right_elbow', 'right_wrist'),\n",
        "\n",
        "        self._get_distance_by_names(landmarks, 'left_hip', 'left_knee'),\n",
        "        self._get_distance_by_names(landmarks, 'right_hip', 'right_knee'),\n",
        "\n",
        "        self._get_distance_by_names(landmarks, 'left_knee', 'left_ankle'),\n",
        "        self._get_distance_by_names(landmarks, 'right_knee', 'right_ankle'),\n",
        "\n",
        "        # Two joints.\n",
        "\n",
        "        self._get_distance_by_names(landmarks, 'left_shoulder', 'left_wrist'),\n",
        "        self._get_distance_by_names(landmarks, 'right_shoulder', 'right_wrist'),\n",
        "\n",
        "        self._get_distance_by_names(landmarks, 'left_hip', 'left_ankle'),\n",
        "        self._get_distance_by_names(landmarks, 'right_hip', 'right_ankle'),\n",
        "\n",
        "        # Three joints.\n",
        "\n",
        "        self._get_distance_by_names(landmarks, 'left_hip', 'left_wrist'),\n",
        "        self._get_distance_by_names(landmarks, 'right_hip', 'right_wrist'),\n",
        "\n",
        "        self._get_distance_by_names(landmarks, 'left_elbow', 'left_knee'),\n",
        "        self._get_distance_by_names(landmarks, 'right_elbow', 'right_knee'),\n",
        "\n",
        "        self._get_distance_by_names(landmarks, 'left_shoulder', 'left_ankle'),\n",
        "        self._get_distance_by_names(landmarks, 'right_shoulder', 'right_ankle'),\n",
        "\n",
        "        # Cross body.\n",
        "\n",
        "        self._get_distance_by_names(landmarks, 'left_elbow', 'right_elbow'),\n",
        "        self._get_distance_by_names(landmarks, 'left_knee', 'right_knee'),\n",
        "\n",
        "        self._get_distance_by_names(landmarks, 'left_wrist', 'right_wrist'),\n",
        "        self._get_distance_by_names(landmarks, 'left_ankle', 'right_ankle'),\n",
        "\n",
        "        # Body bent direction.\n",
        "\n",
        "        self._get_distance(\n",
        "            self._get_average_by_names(landmarks, 'left_wrist', 'left_ankle'),\n",
        "            landmarks[self._landmark_names.index('left_hip')]),\n",
        "        self._get_distance(\n",
        "            self._get_average_by_names(landmarks, 'right_wrist', 'right_ankle'),\n",
        "            landmarks[self._landmark_names.index('right_hip')]),\n",
        "    ])\n",
        "\n",
        "    # embedding = np.array([\n",
        "    #     # One side.\n",
        "    #     self._get_angle_by_names(landmarks, 'left_wrist', 'left_elbow', 'left_shoulder'),\n",
        "    #     self._get_angle_by_names(landmarks, 'right_wrist', 'right_elbow', 'right_shoulder'),\n",
        "    #     self._get_angle_by_names(landmarks, 'left_shoulder', 'left_hip', 'left_knee'),\n",
        "    #     self._get_angle_by_names(landmarks, 'right_shoulder', 'right_hip', 'right_knee'),\n",
        "    #     self._get_angle_by_names(landmarks, 'left_hip', 'left_knee', 'left_ankle'),\n",
        "    #     self._get_angle_by_names(landmarks, 'right_hip', 'right_knee', 'right_ankle'),\n",
        "    #     self._get_angle_by_names(landmarks, 'left_elbow', 'left_shoulder', 'left_hip'),\n",
        "    #     self._get_angle_by_names(landmarks, 'right_elbow', 'right_shoulder', 'right_hip'),\n",
        "        \n",
        "    #     # Cross body.\n",
        "    #     self._get_angle_by_names(landmarks, 'left_shoulder', 'right_shoulder', 'right_hip'),\n",
        "    #     self._get_angle_by_names(landmarks, 'right_shoulder', 'left_shoulder', 'left_hip'),\n",
        "    #     self._get_angle_by_names(landmarks, 'left_shoulder', 'left_hip', 'right_hip'),\n",
        "    #     self._get_angle_by_names(landmarks, 'right_shoulder', 'right_hip', 'left_hip'),\n",
        "    #     self._get_angle_by_names(landmarks, 'left_hip', 'right_hip', 'right_knee'),\n",
        "    #     self._get_angle_by_names(landmarks, 'right_hip', 'left_hip', 'left_knee'),\n",
        "    # ])\n",
        "\n",
        "    return embedding\n",
        "\n",
        "  def _get_average_by_names(self, landmarks, name_from, name_to):\n",
        "    lmk_from = landmarks[self._landmark_names.index(name_from)]\n",
        "    lmk_to = landmarks[self._landmark_names.index(name_to)]\n",
        "    return (lmk_from + lmk_to) * 0.5\n",
        "\n",
        "  def _get_distance_by_names(self, landmarks, name_from, name_to):\n",
        "    lmk_from = landmarks[self._landmark_names.index(name_from)]\n",
        "    lmk_to = landmarks[self._landmark_names.index(name_to)]\n",
        "    return self._get_distance(lmk_from, lmk_to)\n",
        "  \n",
        "  def _get_distance(self, lmk_from, lmk_to):\n",
        "    return lmk_to - lmk_from\n",
        "  \n",
        "  def _get_angle_by_names(self, landmarks, name_from, name_center, name_to):\n",
        "    lmk_from = landmarks[self._landmark_names.index(name_from)]\n",
        "    lmk_center = landmarks[self._landmark_names.index(name_center)]\n",
        "    lmk_to = landmarks[self._landmark_names.index(name_to)]\n",
        "    return self._get_angle(lmk_from, lmk_center, lmk_to)\n",
        "\n",
        "  def _get_angle(self, lmk_from, lmk_center, lmk_to):\n",
        "    vec_from = (lmk_from - lmk_center) * (1., 1., 0.2)\n",
        "    vec_to = (lmk_to - lmk_center) * (1., 1., 0.2)\n",
        "    \n",
        "    return np.dot(vec_from, vec_to) / (np.linalg.norm(vec_from) * np.linalg.norm(vec_to))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hH-efWS61Tfy"
      },
      "source": [
        "## Pose classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TR9M0uAK1WzX"
      },
      "outputs": [],
      "source": [
        "class PoseSample(object):\n",
        "\n",
        "  def __init__(self, name, landmarks, class_name, embedding):\n",
        "    self.name = name\n",
        "    self.landmarks = landmarks\n",
        "    self.class_name = class_name\n",
        "    self.embedding = embedding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class Net_3(nn.Module):\n",
        "  def __init__(self,input,H0,H1,output):\n",
        "    super(Net_3,self).__init__()\n",
        "    self.linear1=nn.Linear(input,H0)\n",
        "    self.linear2=nn.Linear(H0,H1)\n",
        "    self.linear3=nn.Linear(H1,output)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    x=torch.sigmoid(self.linear1(x))  \n",
        "    x=torch.sigmoid(self.linear2(x))  \n",
        "    x=torch.sigmoid(self.linear3(x))\n",
        "    return x\n",
        "\n",
        "import torch.nn as nn\n",
        "class Net_2(nn.Module):\n",
        "  def __init__(self,input,H,output):\n",
        "    super(Net_2,self).__init__()\n",
        "    self.linear1=nn.Linear(input,H)\n",
        "    self.linear2=nn.Linear(H,output)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    x=torch.sigmoid(self.linear1(x))  \n",
        "    x=torch.sigmoid(self.linear2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "wokM2PEEhOjx"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "class PoseClassifier(object):\n",
        "  \"\"\"Classifies pose landmarks.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               pose_samples_folder,\n",
        "               pose_embedder,\n",
        "               file_extension='csv',\n",
        "               file_separator=',',\n",
        "               n_landmarks=33,\n",
        "               n_dimensions=3,\n",
        "               axes_weights=(1., 1., 0.2),\n",
        "               batch_size=64):\n",
        "    self._pose_embedder = pose_embedder\n",
        "    self._n_landmarks = n_landmarks\n",
        "    self._n_dimensions = n_dimensions\n",
        "    self._axes_weights = axes_weights\n",
        "\n",
        "    self._pose_samples = self._load_pose_samples(pose_samples_folder,\n",
        "                                                 file_extension,\n",
        "                                                 file_separator,\n",
        "                                                 n_landmarks,\n",
        "                                                 n_dimensions,\n",
        "                                                 pose_embedder)\n",
        "\n",
        "    print('Successfully loaded {} poses.'.format(len(self._pose_samples)))\n",
        "    random.shuffle(self._pose_samples)\n",
        "    \n",
        "\n",
        "    ## for reproducibility\n",
        "    torch.manual_seed(0)\n",
        "    np.random.seed(0)\n",
        "\n",
        "    class_labels = ['burglary', 'normal']\n",
        "    X = np.asarray([pose_sample.embedding for pose_sample in self._pose_samples], dtype=np.float32)\n",
        "    X = X.reshape((X.shape[0], -1))\n",
        "    Y = np.asarray([class_labels.index(pose_sample.class_name) for pose_sample in self._pose_samples], dtype=np.float32)\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
        "    print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
        "\n",
        "    ## Instantiating classifier\n",
        "    input_dim = 25 * 3   # how many Variables are in the dataset\n",
        "    hidden_dim_0 = 128  # hidden layers\n",
        "    hidden_dim_1 = 32\n",
        "    output_dim = 1   # number of classes\n",
        "    self.device = torch.device('cuda')\n",
        "    self.net = Net_2(input_dim, hidden_dim_0,output_dim).to(self.device)\n",
        "\n",
        "    class Data(Dataset):\n",
        "        def __init__(self, X, Y):\n",
        "            self.X=torch.from_numpy(X)\n",
        "            self.Y=torch.from_numpy(Y)\n",
        "            self.len=self.X.shape[0]\n",
        "        def __getitem__(self,index):      \n",
        "            return self.X[index], self.Y[index]\n",
        "        def __len__(self):\n",
        "            return self.len\n",
        "    \n",
        "    self.train_loader=DataLoader(dataset=Data(X_train,Y_train),batch_size=batch_size)\n",
        "    self.test_loader=DataLoader(dataset=Data(X_test,Y_test),batch_size=batch_size)\n",
        "\n",
        "  def _load_pose_samples(self,\n",
        "                         pose_samples_folder,\n",
        "                         file_extension,\n",
        "                         file_separator,\n",
        "                         n_landmarks,\n",
        "                         n_dimensions,\n",
        "                         pose_embedder):\n",
        "    \"\"\"Loads pose samples from a given folder.\n",
        "    \n",
        "    Required folder structure:\n",
        "      normal.csv\n",
        "      burglary.csv\n",
        "      ...\n",
        "\n",
        "    Required CSV structure:\n",
        "      sample_00001,x1,y1,z1,x2,y2,z2,....\n",
        "      sample_00002,x1,y1,z1,x2,y2,z2,....\n",
        "      ...\n",
        "    \"\"\"\n",
        "    # Each file in the folder represents one pose class.\n",
        "    file_names = [name for name in os.listdir(pose_samples_folder) if name.endswith(file_extension)]\n",
        "\n",
        "    pose_samples = []\n",
        "    for file_name in file_names:\n",
        "      # Use file name as pose class name.\n",
        "      class_name = file_name[:-(len(file_extension) + 1)]\n",
        "      \n",
        "      # Parse CSV.\n",
        "      with open(os.path.join(pose_samples_folder, file_name)) as csv_file:\n",
        "        csv_reader = csv.reader(csv_file, delimiter=file_separator)\n",
        "        for row in csv_reader:\n",
        "          assert len(row) == n_landmarks * n_dimensions + 1, 'Wrong number of values: {}'.format(len(row))\n",
        "          landmarks = np.array(row[1:], np.float32).reshape([n_landmarks, n_dimensions])\n",
        "          pose_samples.append(PoseSample(\n",
        "              name=row[0],\n",
        "              landmarks=landmarks,\n",
        "              class_name=class_name,\n",
        "              embedding=pose_embedder(landmarks),\n",
        "          ))\n",
        "\n",
        "    return pose_samples\n",
        "  \n",
        "  def train_classifier(self, lr=0.01, num_epochs=100):\n",
        "    \n",
        "    ## Defining optimizer and loss function\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.SGD(self.net.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "    ## Defining Training Parameters\n",
        "    running_loss_list = [] # list to store running loss in the code below\n",
        "\n",
        "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(self.train_loader):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = self.net(inputs)\n",
        "            loss = criterion(outputs, labels.unsqueeze(1))\n",
        "            \n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "        if epoch % 100 == 99:\n",
        "            print('[{}, {}] loss: {:.5f}'.format(epoch + 1, i + 1, running_loss / len(self.train_loader)))\n",
        "            # self.test()\n",
        "        running_loss_list.append(running_loss)\n",
        "        running_loss = 0.0\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "    # plot loss curve\n",
        "    # step = np.linspace(0,num_epochs,min(num_epochs, 1000))\n",
        "    # plt.plot(step,np.array(running_loss_list))\n",
        "\n",
        "  def test(self, model_path='./net.pth'):\n",
        "    # net = Net()\n",
        "    # net.load_state_dict(torch.load(model_path))\n",
        "    # uncomment if batchnorm layers or dropout layers are used\n",
        "    # net.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in self.test_loader:\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "            outputs = self.net(inputs)\n",
        "\n",
        "            predicted = torch.round(outputs).reshape(-1)\n",
        "            labels = labels.float()\n",
        "            total += labels.size(0)\n",
        "            \n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    ### complete the code to compute accuracy and store it as the variable acc \n",
        "    acc = correct / total ## stores the accuracy computed in the above loop \n",
        "    print('Accuracy of the network on the test poses: %f %%' % (acc * 100))\n"
      ],
      "metadata": {
        "id": "xS5tvcTbP4Z1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-VRo98tE1JH"
      },
      "source": [
        "## Classification smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "POC4_eQsE3VO"
      },
      "outputs": [],
      "source": [
        "class EMADictSmoothing(object):\n",
        "  \"\"\"Smoothes pose classification.\"\"\"\n",
        "\n",
        "  def __init__(self, window_size=10, alpha=0.2):\n",
        "    self._window_size = window_size\n",
        "    self._alpha = alpha\n",
        "\n",
        "    self._data_in_window = []\n",
        "\n",
        "  def __call__(self, data):\n",
        "    \"\"\"Smoothes given pose classification.\n",
        "\n",
        "    Smoothing is done by computing Exponential Moving Average for every pose\n",
        "    class observed in the given time window. Missed pose classes arre replaced\n",
        "    with 0.\n",
        "    \n",
        "    Args:\n",
        "      data: Dictionary with pose classification. Sample:\n",
        "          {\n",
        "            'normal': 8,\n",
        "            'burglary': 2,\n",
        "          }\n",
        "\n",
        "    Result:\n",
        "      Dictionary in the same format but with smoothed and float instead of\n",
        "      integer values. Sample:\n",
        "        {\n",
        "          'normal': 8.3,\n",
        "          'burglary': 1.7,\n",
        "        }\n",
        "    \"\"\"\n",
        "    # Add new data to the beginning of the window for simpler code.\n",
        "    self._data_in_window.insert(0, data)\n",
        "    self._data_in_window = self._data_in_window[:self._window_size]\n",
        "\n",
        "    # Get all keys.\n",
        "    keys = set([key for data in self._data_in_window for key, _ in data.items()])\n",
        "\n",
        "    # Get smoothed values.\n",
        "    smoothed_data = dict()\n",
        "    for key in keys:\n",
        "      factor = 1.0\n",
        "      top_sum = 0.0\n",
        "      bottom_sum = 0.0\n",
        "      for data in self._data_in_window:\n",
        "        value = data[key] if key in data else 0.0\n",
        "\n",
        "        top_sum += factor * value\n",
        "        bottom_sum += factor\n",
        "\n",
        "        # Update factor.\n",
        "        factor *= (1.0 - self._alpha)\n",
        "\n",
        "      smoothed_data[key] = top_sum / bottom_sum\n",
        "\n",
        "    return smoothed_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWuA2OYgGtZn"
      },
      "source": [
        "## Repetition counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TEs_lgNiGv-j"
      },
      "outputs": [],
      "source": [
        "class RepetitionCounter(object):\n",
        "  \"\"\"Counts number of repetitions of given target pose class.\"\"\"\n",
        "\n",
        "  def __init__(self, class_name, enter_threshold=6, exit_threshold=4):\n",
        "    self._class_name = class_name\n",
        "\n",
        "    # If pose counter passes given threshold, then we enter the pose.\n",
        "    self._enter_threshold = enter_threshold\n",
        "    self._exit_threshold = exit_threshold\n",
        "\n",
        "    # Either we are in given pose or not.\n",
        "    self._pose_entered = False\n",
        "\n",
        "    # Number of times we exited the pose.\n",
        "    self._n_repeats = 0\n",
        "\n",
        "  @property\n",
        "  def n_repeats(self):\n",
        "    return self._n_repeats\n",
        "\n",
        "  def __call__(self, pose_classification):\n",
        "    \"\"\"Counts number of repetitions happend until given frame.\n",
        "\n",
        "    We use two thresholds. First you need to go above the higher one to enter\n",
        "    the pose, and then you need to go below the lower one to exit it. Difference\n",
        "    between the thresholds makes it stable to prediction jittering (which will\n",
        "    cause wrong counts in case of having only one threshold).\n",
        "    \n",
        "    Args:\n",
        "      pose_classification: Pose classification dictionary on current frame.\n",
        "        Sample:\n",
        "          {\n",
        "            'normal': 8.3,\n",
        "            'burglary': 1.7,\n",
        "          }\n",
        "\n",
        "    Returns:\n",
        "      Integer counter of repetitions.\n",
        "    \"\"\"\n",
        "    # Get pose confidence.\n",
        "    pose_confidence = 0.0\n",
        "    if self._class_name in pose_classification:\n",
        "      pose_confidence = pose_classification[self._class_name]\n",
        "\n",
        "    # On the very first frame or if we were out of the pose, just check if we\n",
        "    # entered it on this frame and update the state.\n",
        "    if not self._pose_entered:\n",
        "      self._pose_entered = pose_confidence > self._enter_threshold\n",
        "      return self._n_repeats\n",
        "\n",
        "    # If we were in the pose and are exiting it, then increase the counter and\n",
        "    # update the state.\n",
        "    if pose_confidence < self._exit_threshold:\n",
        "      self._n_repeats += 1\n",
        "      self._pose_entered = False\n",
        "\n",
        "    return self._n_repeats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIiEj8Tx_x-q"
      },
      "source": [
        "# Step 1: Build classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QBS_P_Y_2mg"
      },
      "source": [
        "## Bootstrap images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUVYNl9RWsmW",
        "outputId": "da16e455-e400-412f-9d6d-04555f02214f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/My Drive/Colab Notebooks/Capstone\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "%cd /content/drive/My Drive/Colab Notebooks/Capstone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bERVPO8Ja6j7"
      },
      "outputs": [],
      "source": [
        "# Required structure of the images_in_folder:\n",
        "#\n",
        "#   poses_images_in/\n",
        "#     burglary/\n",
        "#       image_001.jpg\n",
        "#       image_002.jpg\n",
        "#       ...\n",
        "#     normal/\n",
        "#       image_001.jpg\n",
        "#       image_002.jpg\n",
        "#       ...\n",
        "#     ...\n",
        "bootstrap_images_in_folder = 'poses_images_in'\n",
        "\n",
        "# Output folders for bootstrapped images and CSVs.\n",
        "bootstrap_images_out_folder = 'poses_images_out'\n",
        "bootstrap_csvs_out_folder = 'poses_csvs_out'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "nQbiwhbrY3ZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforms pose landmarks into embedding.\n",
        "pose_embedder = FullBodyPoseEmbedder()\n",
        "\n",
        "\n",
        "\n",
        "for lr in [0.1, 0.05, 0.01]:\n",
        "    for batch_size in [100, 250, 500, 1000]:\n",
        "        print(lr, batch_size)\n",
        "        # Classifies give pose against database of poses.\n",
        "        pose_classifier = PoseClassifier(\n",
        "                pose_samples_folder=bootstrap_csvs_out_folder,\n",
        "                pose_embedder=pose_embedder,\n",
        "                batch_size=batch_size)\n",
        "        pose_classifier.train_classifier(lr=lr,num_epochs=1000)\n",
        "        pose_classifier.test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uwZWok9Y-Nc",
        "outputId": "71950e8d-7a1b-481a-e180-f757d4530961"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1 100\n",
            "Successfully loaded 2367 poses.\n",
            "(1893, 75) (474, 75) (1893,) (474,)\n",
            "[100, 19] loss: 0.29507\n",
            "[200, 19] loss: 0.24490\n",
            "[300, 19] loss: 0.18009\n",
            "[400, 19] loss: 0.17431\n",
            "[500, 19] loss: 0.15208\n",
            "[600, 19] loss: 0.18103\n",
            "[700, 19] loss: 0.17999\n",
            "[800, 19] loss: 0.12991\n",
            "[900, 19] loss: 0.11245\n",
            "[1000, 19] loss: 0.16405\n",
            "Finished Training\n",
            "Accuracy of the network on the test poses: 80.590717 %\n",
            "0.1 250\n",
            "Successfully loaded 2367 poses.\n",
            "(1893, 75) (474, 75) (1893,) (474,)\n",
            "[100, 8] loss: 0.14279\n",
            "[200, 8] loss: 0.05407\n",
            "[300, 8] loss: 0.03195\n",
            "[400, 8] loss: 0.02248\n",
            "[500, 8] loss: 0.01824\n",
            "[600, 8] loss: 0.01377\n",
            "[700, 8] loss: 0.01157\n",
            "[800, 8] loss: 0.00999\n",
            "[900, 8] loss: 0.00876\n",
            "[1000, 8] loss: 0.00781\n",
            "Finished Training\n",
            "Accuracy of the network on the test poses: 83.966245 %\n",
            "0.1 500\n",
            "Successfully loaded 2367 poses.\n",
            "(1893, 75) (474, 75) (1893,) (474,)\n",
            "[100, 4] loss: 0.20430\n",
            "[200, 4] loss: 0.11204\n",
            "[300, 4] loss: 0.06985\n",
            "[400, 4] loss: 0.04991\n",
            "[500, 4] loss: 0.03820\n",
            "[600, 4] loss: 0.03045\n",
            "[700, 4] loss: 0.02522\n",
            "[800, 4] loss: 0.02118\n",
            "[900, 4] loss: 0.01817\n",
            "[1000, 4] loss: 0.01588\n",
            "Finished Training\n",
            "Accuracy of the network on the test poses: 86.286920 %\n",
            "0.1 1000\n",
            "Successfully loaded 2367 poses.\n",
            "(1893, 75) (474, 75) (1893,) (474,)\n",
            "[100, 2] loss: 0.33288\n",
            "[200, 2] loss: 0.19563\n",
            "[300, 2] loss: 0.13433\n",
            "[400, 2] loss: 0.09975\n",
            "[500, 2] loss: 0.07777\n",
            "[600, 2] loss: 0.06335\n",
            "[700, 2] loss: 0.05295\n",
            "[800, 2] loss: 0.04472\n",
            "[900, 2] loss: 0.03858\n",
            "[1000, 2] loss: 0.03378\n",
            "Finished Training\n",
            "Accuracy of the network on the test poses: 87.552743 %\n",
            "0.05 100\n",
            "Successfully loaded 2367 poses.\n",
            "(1893, 75) (474, 75) (1893,) (474,)\n",
            "[100, 19] loss: 0.26528\n",
            "[200, 19] loss: 0.30980\n",
            "[300, 19] loss: 0.29045\n",
            "[400, 19] loss: 0.26397\n",
            "[500, 19] loss: 0.18482\n",
            "[600, 19] loss: 0.21968\n",
            "[700, 19] loss: 0.18680\n",
            "[800, 19] loss: 0.15193\n",
            "[900, 19] loss: 0.11479\n",
            "[1000, 19] loss: 0.16511\n",
            "Finished Training\n",
            "Accuracy of the network on the test poses: 82.489451 %\n",
            "0.05 250\n",
            "Successfully loaded 2367 poses.\n",
            "(1893, 75) (474, 75) (1893,) (474,)\n",
            "[100, 8] loss: 0.12874\n",
            "[200, 8] loss: 0.05934\n",
            "[300, 8] loss: 0.03134\n",
            "[400, 8] loss: 0.02229\n",
            "[500, 8] loss: 0.01649\n",
            "[600, 8] loss: 0.01329\n",
            "[700, 8] loss: 0.01108\n",
            "[800, 8] loss: 0.00953\n",
            "[900, 8] loss: 0.00836\n",
            "[1000, 8] loss: 0.00744\n",
            "Finished Training\n",
            "Accuracy of the network on the test poses: 87.763713 %\n",
            "0.05 500\n",
            "Successfully loaded 2367 poses.\n",
            "(1893, 75) (474, 75) (1893,) (474,)\n",
            "[100, 4] loss: 0.20681\n",
            "[200, 4] loss: 0.10856\n",
            "[300, 4] loss: 0.06959\n",
            "[400, 4] loss: 0.04993\n",
            "[500, 4] loss: 0.03845\n",
            "[600, 4] loss: 0.03085\n",
            "[700, 4] loss: 0.02580\n",
            "[800, 4] loss: 0.02203\n",
            "[900, 4] loss: 0.01914\n",
            "[1000, 4] loss: 0.01696\n",
            "Finished Training\n",
            "Accuracy of the network on the test poses: 88.185654 %\n",
            "0.05 1000\n",
            "Successfully loaded 2367 poses.\n",
            "(1893, 75) (474, 75) (1893,) (474,)\n",
            "[100, 2] loss: 0.32466\n",
            "[200, 2] loss: 0.19189\n",
            "[300, 2] loss: 0.13345\n",
            "[400, 2] loss: 0.10055\n",
            "[500, 2] loss: 0.07964\n",
            "[600, 2] loss: 0.06552\n",
            "[700, 2] loss: 0.05524\n",
            "[800, 2] loss: 0.04732\n",
            "[900, 2] loss: 0.04150\n",
            "[1000, 2] loss: 0.03692\n",
            "Finished Training\n",
            "Accuracy of the network on the test poses: 85.864979 %\n",
            "0.01 100\n",
            "Successfully loaded 2367 poses.\n",
            "(1893, 75) (474, 75) (1893,) (474,)\n",
            "[100, 19] loss: 0.30063\n",
            "[200, 19] loss: 0.22084\n",
            "[300, 19] loss: 0.20787\n",
            "[400, 19] loss: 0.17648\n",
            "[500, 19] loss: 0.20927\n",
            "[600, 19] loss: 0.14406\n",
            "[700, 19] loss: 0.11194\n",
            "[800, 19] loss: 0.19164\n",
            "[900, 19] loss: 0.13634\n",
            "[1000, 19] loss: 0.09113\n",
            "Finished Training\n",
            "Accuracy of the network on the test poses: 84.388186 %\n",
            "0.01 250\n",
            "Successfully loaded 2367 poses.\n",
            "(1893, 75) (474, 75) (1893,) (474,)\n",
            "[100, 8] loss: 0.13334\n",
            "[200, 8] loss: 0.06757\n",
            "[300, 8] loss: 0.03790\n",
            "[400, 8] loss: 0.02361\n",
            "[500, 8] loss: 0.01804\n",
            "[600, 8] loss: 0.01437\n",
            "[700, 8] loss: 0.01195\n",
            "[800, 8] loss: 0.01007\n",
            "[900, 8] loss: 0.00874\n",
            "[1000, 8] loss: 0.00775\n",
            "Finished Training\n",
            "Accuracy of the network on the test poses: 87.974684 %\n",
            "0.01 500\n",
            "Successfully loaded 2367 poses.\n",
            "(1893, 75) (474, 75) (1893,) (474,)\n",
            "[100, 4] loss: 0.20444\n",
            "[200, 4] loss: 0.10581\n",
            "[300, 4] loss: 0.06874\n",
            "[400, 4] loss: 0.04839\n",
            "[500, 4] loss: 0.03692\n",
            "[600, 4] loss: 0.02976\n",
            "[700, 4] loss: 0.02467\n",
            "[800, 4] loss: 0.02102\n",
            "[900, 4] loss: 0.01830\n",
            "[1000, 4] loss: 0.01618\n",
            "Finished Training\n",
            "Accuracy of the network on the test poses: 87.552743 %\n",
            "0.01 1000\n",
            "Successfully loaded 2367 poses.\n",
            "(1893, 75) (474, 75) (1893,) (474,)\n",
            "[100, 2] loss: 0.33222\n",
            "[200, 2] loss: 0.19584\n",
            "[300, 2] loss: 0.13765\n",
            "[400, 2] loss: 0.10435\n",
            "[500, 2] loss: 0.08223\n",
            "[600, 2] loss: 0.06742\n",
            "[700, 2] loss: 0.05678\n",
            "[800, 2] loss: 0.04867\n",
            "[900, 2] loss: 0.04237\n",
            "[1000, 2] loss: 0.03738\n",
            "Finished Training\n",
            "Accuracy of the network on the test poses: 86.497890 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfE7C9wHel_7"
      },
      "source": [
        "# Step 2: Classification\n",
        "\n",
        "**Important!!** Check that you are using the same classification parameters as while building classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4eaOJpoEvFl",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "outputId": "e6977023-79ed-4899-d8ce-c3a66e194f0f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e6dbcfd8-0d62-403e-a880-a4cc1f39641e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e6dbcfd8-0d62-403e-a880-a4cc1f39641e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Burglary066_x264.mp4 to Burglary066_x264.mp4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Pose classification (basic).ipynb',\n",
              " 'poses_images_in',\n",
              " 'poses_csvs_out',\n",
              " 'poses_images_out',\n",
              " 'Pose classification (extended).ipynb',\n",
              " 'Burglary066_x264.mp4']"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "# Upload your video.\n",
        "uploaded = files.upload()\n",
        "os.listdir('.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYkZJ3a_2MW_"
      },
      "outputs": [],
      "source": [
        "# Specify your video name and target pose class to count the repetitions.\n",
        "video_path = 'Burglary066_x264.mp4'\n",
        "class_name='burglary'\n",
        "out_video_path = 'sample-out.mov'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3gGMDE0R2pe"
      },
      "outputs": [],
      "source": [
        "# Open the video.\n",
        "import cv2\n",
        "\n",
        "video_cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get some video parameters to generate output video with classificaiton.\n",
        "video_n_frames = video_cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "video_fps = video_cap.get(cv2.CAP_PROP_FPS)\n",
        "video_width = int(video_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "video_height = int(video_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7t_ACEmTSOhr"
      },
      "outputs": [],
      "source": [
        "# Initilize tracker, classifier and counter.\n",
        "# Do that before every video as all of them have state.\n",
        "from mediapipe.python.solutions import pose as mp_pose\n",
        "\n",
        "\n",
        "# Folder with pose class CSVs. That should be the same folder you using while\n",
        "# building classifier to output CSVs.\n",
        "pose_samples_folder = 'poses_csvs_out'\n",
        "\n",
        "# Initialize tracker.\n",
        "pose_tracker = mp_pose.Pose()\n",
        "\n",
        "# Initialize embedder.\n",
        "pose_embedder = FullBodyPoseEmbedder()\n",
        "\n",
        "# Initialize classifier.\n",
        "# Check that you are using the same parameters as during bootstrapping.\n",
        "pose_classifier = PoseClassifier(\n",
        "    pose_samples_folder=pose_samples_folder,\n",
        "    pose_embedder=pose_embedder,\n",
        "    top_n_by_max_distance=30,\n",
        "    top_n_by_mean_distance=10)\n",
        "\n",
        "# # Uncomment to validate target poses used by classifier and find outliers.\n",
        "# outliers = pose_classifier.find_pose_sample_outliers()\n",
        "# print('Number of pose sample outliers (consider removing them): ', len(outliers))\n",
        "\n",
        "# Initialize EMA smoothing.\n",
        "pose_classification_filter = EMADictSmoothing(\n",
        "    window_size=10,\n",
        "    alpha=0.2)\n",
        "\n",
        "# Initialize counter.\n",
        "repetition_counter = RepetitionCounter(\n",
        "    class_name=class_name,\n",
        "    enter_threshold=6,\n",
        "    exit_threshold=4)\n",
        "\n",
        "# Initialize renderer.\n",
        "\n",
        " = PoseClassificationVisualizer(\n",
        "    class_name=class_name,\n",
        "    plot_x_max=video_n_frames,\n",
        "    # Graphic looks nicer if it's the same as `top_n_by_mean_distance`.\n",
        "    plot_y_max=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lXymkneOjgZ"
      },
      "outputs": [],
      "source": [
        "# Run classification on a video.\n",
        "import os\n",
        "import tqdm\n",
        "\n",
        "from mediapipe.python.solutions import drawing_utils as mp_drawing\n",
        "\n",
        "\n",
        "# Open output video.\n",
        "out_video = cv2.VideoWriter(out_video_path, cv2.VideoWriter_fourcc(*'mp4v'), video_fps, (video_width, video_height))\n",
        "\n",
        "frame_idx = 0\n",
        "output_frame = None\n",
        "with tqdm.tqdm(total=video_n_frames, position=0, leave=True) as pbar:\n",
        "  while True:\n",
        "    # Get next frame of the video.\n",
        "    success, input_frame = video_cap.read()\n",
        "    if not success:\n",
        "      break\n",
        "\n",
        "    # Run pose tracker.\n",
        "    input_frame = cv2.cvtColor(input_frame, cv2.COLOR_BGR2RGB)\n",
        "    result = pose_tracker.process(image=input_frame)\n",
        "    pose_landmarks = result.pose_landmarks\n",
        "\n",
        "    # Draw pose prediction.\n",
        "    output_frame = input_frame.copy()\n",
        "    if pose_landmarks is not None:\n",
        "      mp_drawing.draw_landmarks(\n",
        "          image=output_frame,\n",
        "          landmark_list=pose_landmarks,\n",
        "          connections=mp_pose.POSE_CONNECTIONS)\n",
        "    \n",
        "    if pose_landmarks is not None:\n",
        "      # Get landmarks.\n",
        "      frame_height, frame_width = output_frame.shape[0], output_frame.shape[1]\n",
        "      pose_landmarks = np.array([[lmk.x * frame_width, lmk.y * frame_height, lmk.z * frame_width]\n",
        "                                 for lmk in pose_landmarks.landmark], dtype=np.float32)\n",
        "      assert pose_landmarks.shape == (33, 3), 'Unexpected landmarks shape: {}'.format(pose_landmarks.shape)\n",
        "\n",
        "      # Classify the pose on the current frame.\n",
        "      pose_classification = pose_classifier(pose_landmarks)\n",
        "\n",
        "      # Smooth classification using EMA.\n",
        "      pose_classification_filtered = pose_classification_filter(pose_classification)\n",
        "\n",
        "      # Count repetitions.\n",
        "      repetitions_count = repetition_counter(pose_classification_filtered)\n",
        "    else:\n",
        "      # No pose => no classification on current frame.\n",
        "      pose_classification = None\n",
        "\n",
        "      # Still add empty classification to the filter to maintaing correct\n",
        "      # smoothing for future frames.\n",
        "      pose_classification_filtered = pose_classification_filter(dict())\n",
        "      pose_classification_filtered = None\n",
        "\n",
        "      # Don't update the counter presuming that person is 'frozen'. Just\n",
        "      # take the latest repetitions count.\n",
        "      repetitions_count = repetition_counter.n_repeats\n",
        "\n",
        "    # Draw classification plot and repetition counter.\n",
        "    output_frame = pose_classification_visualizer(\n",
        "        frame=output_frame,\n",
        "        pose_classification=pose_classification,\n",
        "        pose_classification_filtered=pose_classification_filtered,\n",
        "        repetitions_count=repetitions_count)\n",
        "\n",
        "    # Save the output frame.\n",
        "    out_video.write(cv2.cvtColor(np.array(output_frame), cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    # Show intermediate frames of the video to track progress.\n",
        "    if frame_idx % 50 == 0:\n",
        "      show_image(output_frame)\n",
        "\n",
        "    frame_idx += 1\n",
        "    pbar.update()\n",
        "\n",
        "# Close output video.\n",
        "out_video.release()\n",
        "\n",
        "# Release MediaPipe resources.\n",
        "pose_tracker.close()\n",
        "\n",
        "# Show the last frame of the video.\n",
        "if output_frame is not None:\n",
        "  show_image(output_frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJFftPiQE56E"
      },
      "outputs": [],
      "source": [
        "# Download generated video\n",
        "files.download(out_video_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Pose classification (NN).ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}